{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afe4de0-ce60-4e7e-a6e9-bcc8d8171bfd",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "#It is a ML technique that envovles combining mutiple individual models known as base learners to make predictions or decisions .\n",
    "\n",
    "The main idea behind ensemble learning is that by combining the predictions of multiple models .\n",
    "\n",
    "The overall performance can be improved compared to using a single model .\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c725a-5c30-4ebe-a336-3e23a2390538",
   "metadata": {},
   "source": [
    "# Techniques \n",
    "1. Bagging\n",
    "2. Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb77009-e7e0-4d07-b5b9-6189d5ccca05",
   "metadata": {},
   "source": [
    "# 1. Bagging\n",
    "It stands for Boot Strap aggregating , It envolves creating multiple subsets of the training data through boot straping .\n",
    "\n",
    "Training a seaparate base learner on each subset and then combining thier predections .\n",
    "\n",
    "The most common ex. of bagging is Randomforest algorithm which combine multiple Decision trees.\n",
    "\n",
    "# 2. Boosting \n",
    "It is an iterative ensemble method that focus on training weak learners sequentially and giving more importance to the instances that were \n",
    "missclassified by the previous learners.\n",
    "\n",
    "In boosting each base learner is trained to correct the mistakes made by the previous learners .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055fa45-d1d6-476e-8be5-5d4632b411a3",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "Bagging: High-variance models (e.g., decision trees)\n",
    "\n",
    "Boosting: High-bias models (e.g., linear classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96788fdd-f6cf-4e6b-8129-358f0ebbf1d3",
   "metadata": {},
   "source": [
    "# Advantange of Ensemble learning \n",
    "1. Improved performance ---> ensemble methods can often achieve better performance than individual models specially when the base learners are diversed and complimentary.\n",
    "2. Robustness ---> It can be more robust to noisy data and outliers because erros made by individual models can be compensated by others.\n",
    "3. Reducing overfitting ---> It can help reduce overfitting as the combination of multiple models reduces the risk of relying to heavly on a single model .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f4fee-b774-408e-9a59-7c2b4cc10efc",
   "metadata": {},
   "source": [
    "# Bagging - ensemble learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b603683-810f-4480-b4d5-d42cd008b3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "x, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Initialize a list to store the base learners\n",
    "base_learners = []\n",
    "\n",
    "# Number of base learners (decision trees)\n",
    "num_base_learners = 10\n",
    "\n",
    "# Train the base learners\n",
    "for i in range(num_base_learners):\n",
    "    # Create a bootstrap sample of the training data\n",
    "    bootstrap_indices = np.random.choice(len(x_train), size=len(x_train),\n",
    "                                         replace=True)\n",
    "    x_bootstrap = x_train[bootstrap_indices]\n",
    "    y_bootstrap = y_train[bootstrap_indices]\n",
    "\n",
    "    # Create and train a base learner (Random Forest)\n",
    "    base_learner = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    base_learner.fit(x_bootstrap, y_bootstrap)\n",
    "\n",
    "    # Add the trained base learner to the list\n",
    "    base_learners.append(base_learner)\n",
    "\n",
    "# Make predictions with each base learner\n",
    "base_predictions = []\n",
    "for base_learner in base_learners:\n",
    "    y_pred = base_learner.predict(x_test)\n",
    "    base_predictions.append(y_pred)\n",
    "\n",
    "# Combine the predictions using majority voting\n",
    "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095030c-797f-4b62-9d09-011915399a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
