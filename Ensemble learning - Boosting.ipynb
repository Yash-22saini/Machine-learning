{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa5089f8-098d-44bb-b080-167ae6069651",
   "metadata": {},
   "source": [
    "# Boosting :\n",
    "Boosting is a popular technique used in ensemble learning, which combines multiple weak or bas\n",
    "e learners to create a stronger predictive model. The main idea behind boosting is to sequenti\n",
    "ally train a series of models, where each subsequent model focuses on the instances that were\n",
    "misclassified by the previous models. This iterative process allows the ensemble to learn from\n",
    "its mistakes and improve its overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e2c6a-d1d3-4d1f-a8bf-f2f1fdf3a303",
   "metadata": {},
   "source": [
    "# Types of boosting\n",
    "1. Add a boost ----> it assingns higher weights to miss classified instance and focuses on those instances during subsequent iterations .\n",
    "   it sequentially trains a series of weak learners and combine their predictions to form the final ensemble. it is used for binary classification problems.\n",
    "2. Gradient boosting --->  ensemble of weak learners in a stage wise manner , each subsequent model is trained to correct the mistakes made by the previous models by fitting the -ve gradient of the loss function. gradient boosting can handle both classification and regression task and is often used with decision tree as base learners .\n",
    "   1. exsi boost---- it is an optimize iplentation of gradient boosting that offers several enhancements including paralllel proceesing regularization technques and handling missing values . it uses a combination of tree based models and linear models for boosting which allows it to capture both linear and non linear relationships in the data efficiently .\n",
    "   2. light gvm ---- is another gradient boosting framework that focuses on acheving faster training , speed and lower memory uses. it uses a novel tree training called gradient paste  .______________________________\n",
    "   3. cata boost ---- this a gradient boosting algo that is design to handle categorical features directly without the need for extensive data preproceesing . it incorperates and innovative method to handle categorical variables which includes appyling a combination of orederd boosting random permutationns and symmerteric trees,\n",
    "   4. stochastic gradient boosting ----- introduce randomness in the boosting process by sub smampling the training data or features at each iteration . it helps to reduce overfitting and can improve the models generalization ability specially when dealing with large datasets .\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad1783-099c-4457-a59b-60860a0b7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0bfb39-5094-4b78-8583-71cfe4f96e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.85\n",
      "XGboost Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary Libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training an AdaBoost classifier\n",
    "adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Creating and training an XGBoost classifier\n",
    "xgboost = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgboost.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set for AdaBoost\n",
    "y_pred_adaboost = adaboost.predict(X_test)\n",
    "\n",
    "# Making predictions on the test set for XGBoost\n",
    "y_pred_xgboost = xgboost.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of AdaBoost\n",
    "accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n",
    "print(\"AdaBoost Accuracy:\", accuracy_adaboost)\n",
    "\n",
    "# Calculating the accuracy of XGboost\n",
    "accuracy_xgboost = accuracy_score(y_test,y_pred_xgboost)\n",
    "print(\"XGboost Accuracy:\", accuracy_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a769f7-2120-401b-b235-989721f56b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ee9b05-dd43-4581-9a93-8bf425d74e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f77245ad-94a6-4b2f-84dc-499833e0d56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier Accuracy: 0.9\n",
      "[LightGBM] [Info] Number of positive: 388, number of negative: 412\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000449 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2550\n",
      "[LightGBM] [Info] Number of data points in the train set: 800, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.485000 -> initscore=-0.060018\n",
      "[LightGBM] [Info] Start training from score -0.060018\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LGBM Classifier Accuracy: 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.077367\n",
      "0:\tlearn: 0.6558066\ttotal: 4.96ms\tremaining: 492ms\n",
      "1:\tlearn: 0.6159550\ttotal: 8.85ms\tremaining: 434ms\n",
      "2:\tlearn: 0.5898032\ttotal: 12.9ms\tremaining: 418ms\n",
      "3:\tlearn: 0.5613805\ttotal: 16.9ms\tremaining: 407ms\n",
      "4:\tlearn: 0.5370679\ttotal: 20.7ms\tremaining: 394ms\n",
      "5:\tlearn: 0.5163689\ttotal: 24.8ms\tremaining: 388ms\n",
      "6:\tlearn: 0.4914713\ttotal: 29.2ms\tremaining: 388ms\n",
      "7:\tlearn: 0.4727620\ttotal: 33.6ms\tremaining: 387ms\n",
      "8:\tlearn: 0.4584597\ttotal: 38ms\tremaining: 384ms\n",
      "9:\tlearn: 0.4440620\ttotal: 42.3ms\tremaining: 381ms\n",
      "10:\tlearn: 0.4308778\ttotal: 46.8ms\tremaining: 378ms\n",
      "11:\tlearn: 0.4156244\ttotal: 51ms\tremaining: 374ms\n",
      "12:\tlearn: 0.4008058\ttotal: 55.1ms\tremaining: 368ms\n",
      "13:\tlearn: 0.3904452\ttotal: 59.5ms\tremaining: 365ms\n",
      "14:\tlearn: 0.3780935\ttotal: 63ms\tremaining: 357ms\n",
      "15:\tlearn: 0.3688289\ttotal: 67.1ms\tremaining: 352ms\n",
      "16:\tlearn: 0.3578652\ttotal: 71.6ms\tremaining: 350ms\n",
      "17:\tlearn: 0.3482243\ttotal: 75.9ms\tremaining: 346ms\n",
      "18:\tlearn: 0.3407976\ttotal: 80.3ms\tremaining: 342ms\n",
      "19:\tlearn: 0.3346774\ttotal: 84.7ms\tremaining: 339ms\n",
      "20:\tlearn: 0.3262485\ttotal: 88.7ms\tremaining: 334ms\n",
      "21:\tlearn: 0.3204620\ttotal: 93ms\tremaining: 330ms\n",
      "22:\tlearn: 0.3152497\ttotal: 97.2ms\tremaining: 325ms\n",
      "23:\tlearn: 0.3113100\ttotal: 101ms\tremaining: 320ms\n",
      "24:\tlearn: 0.3055197\ttotal: 105ms\tremaining: 314ms\n",
      "25:\tlearn: 0.3023932\ttotal: 108ms\tremaining: 307ms\n",
      "26:\tlearn: 0.2991232\ttotal: 112ms\tremaining: 301ms\n",
      "27:\tlearn: 0.2944204\ttotal: 115ms\tremaining: 297ms\n",
      "28:\tlearn: 0.2902317\ttotal: 120ms\tremaining: 293ms\n",
      "29:\tlearn: 0.2846020\ttotal: 124ms\tremaining: 289ms\n",
      "30:\tlearn: 0.2799371\ttotal: 128ms\tremaining: 285ms\n",
      "31:\tlearn: 0.2756571\ttotal: 132ms\tremaining: 280ms\n",
      "32:\tlearn: 0.2726753\ttotal: 136ms\tremaining: 276ms\n",
      "33:\tlearn: 0.2674082\ttotal: 139ms\tremaining: 270ms\n",
      "34:\tlearn: 0.2631823\ttotal: 142ms\tremaining: 265ms\n",
      "35:\tlearn: 0.2589993\ttotal: 146ms\tremaining: 259ms\n",
      "36:\tlearn: 0.2568804\ttotal: 149ms\tremaining: 253ms\n",
      "37:\tlearn: 0.2538737\ttotal: 152ms\tremaining: 247ms\n",
      "38:\tlearn: 0.2513689\ttotal: 156ms\tremaining: 244ms\n",
      "39:\tlearn: 0.2482830\ttotal: 159ms\tremaining: 239ms\n",
      "40:\tlearn: 0.2463209\ttotal: 163ms\tremaining: 234ms\n",
      "41:\tlearn: 0.2441432\ttotal: 167ms\tremaining: 230ms\n",
      "42:\tlearn: 0.2416619\ttotal: 171ms\tremaining: 226ms\n",
      "43:\tlearn: 0.2396797\ttotal: 175ms\tremaining: 223ms\n",
      "44:\tlearn: 0.2378427\ttotal: 179ms\tremaining: 219ms\n",
      "45:\tlearn: 0.2364648\ttotal: 183ms\tremaining: 214ms\n",
      "46:\tlearn: 0.2350752\ttotal: 186ms\tremaining: 210ms\n",
      "47:\tlearn: 0.2315079\ttotal: 189ms\tremaining: 205ms\n",
      "48:\tlearn: 0.2294301\ttotal: 192ms\tremaining: 200ms\n",
      "49:\tlearn: 0.2281681\ttotal: 196ms\tremaining: 196ms\n",
      "50:\tlearn: 0.2267751\ttotal: 200ms\tremaining: 192ms\n",
      "51:\tlearn: 0.2247380\ttotal: 203ms\tremaining: 188ms\n",
      "52:\tlearn: 0.2228263\ttotal: 207ms\tremaining: 184ms\n",
      "53:\tlearn: 0.2211005\ttotal: 211ms\tremaining: 179ms\n",
      "54:\tlearn: 0.2194083\ttotal: 215ms\tremaining: 176ms\n",
      "55:\tlearn: 0.2179934\ttotal: 219ms\tremaining: 172ms\n",
      "56:\tlearn: 0.2154257\ttotal: 226ms\tremaining: 170ms\n",
      "57:\tlearn: 0.2143137\ttotal: 229ms\tremaining: 166ms\n",
      "58:\tlearn: 0.2129936\ttotal: 233ms\tremaining: 162ms\n",
      "59:\tlearn: 0.2122751\ttotal: 237ms\tremaining: 158ms\n",
      "60:\tlearn: 0.2112648\ttotal: 241ms\tremaining: 154ms\n",
      "61:\tlearn: 0.2104971\ttotal: 245ms\tremaining: 150ms\n",
      "62:\tlearn: 0.2091630\ttotal: 248ms\tremaining: 146ms\n",
      "63:\tlearn: 0.2073148\ttotal: 252ms\tremaining: 142ms\n",
      "64:\tlearn: 0.2045572\ttotal: 256ms\tremaining: 138ms\n",
      "65:\tlearn: 0.2032156\ttotal: 260ms\tremaining: 134ms\n",
      "66:\tlearn: 0.2020471\ttotal: 263ms\tremaining: 130ms\n",
      "67:\tlearn: 0.1988264\ttotal: 268ms\tremaining: 126ms\n",
      "68:\tlearn: 0.1976842\ttotal: 271ms\tremaining: 122ms\n",
      "69:\tlearn: 0.1968678\ttotal: 276ms\tremaining: 118ms\n",
      "70:\tlearn: 0.1951905\ttotal: 280ms\tremaining: 114ms\n",
      "71:\tlearn: 0.1942429\ttotal: 284ms\tremaining: 111ms\n",
      "72:\tlearn: 0.1935453\ttotal: 288ms\tremaining: 107ms\n",
      "73:\tlearn: 0.1929597\ttotal: 292ms\tremaining: 103ms\n",
      "74:\tlearn: 0.1917594\ttotal: 296ms\tremaining: 98.7ms\n",
      "75:\tlearn: 0.1914746\ttotal: 300ms\tremaining: 94.9ms\n",
      "76:\tlearn: 0.1905238\ttotal: 304ms\tremaining: 90.8ms\n",
      "77:\tlearn: 0.1896291\ttotal: 307ms\tremaining: 86.7ms\n",
      "78:\tlearn: 0.1889220\ttotal: 311ms\tremaining: 82.7ms\n",
      "79:\tlearn: 0.1880880\ttotal: 315ms\tremaining: 78.6ms\n",
      "80:\tlearn: 0.1868659\ttotal: 317ms\tremaining: 74.5ms\n",
      "81:\tlearn: 0.1862109\ttotal: 321ms\tremaining: 70.5ms\n",
      "82:\tlearn: 0.1857630\ttotal: 325ms\tremaining: 66.5ms\n",
      "83:\tlearn: 0.1851696\ttotal: 329ms\tremaining: 62.7ms\n",
      "84:\tlearn: 0.1846978\ttotal: 333ms\tremaining: 58.8ms\n",
      "85:\tlearn: 0.1842192\ttotal: 337ms\tremaining: 54.9ms\n",
      "86:\tlearn: 0.1833965\ttotal: 341ms\tremaining: 51ms\n",
      "87:\tlearn: 0.1828304\ttotal: 345ms\tremaining: 47ms\n",
      "88:\tlearn: 0.1817573\ttotal: 349ms\tremaining: 43.1ms\n",
      "89:\tlearn: 0.1812930\ttotal: 353ms\tremaining: 39.2ms\n",
      "90:\tlearn: 0.1802531\ttotal: 358ms\tremaining: 35.4ms\n",
      "91:\tlearn: 0.1792956\ttotal: 362ms\tremaining: 31.4ms\n",
      "92:\tlearn: 0.1786213\ttotal: 366ms\tremaining: 27.5ms\n",
      "93:\tlearn: 0.1784045\ttotal: 370ms\tremaining: 23.6ms\n",
      "94:\tlearn: 0.1781292\ttotal: 374ms\tremaining: 19.7ms\n",
      "95:\tlearn: 0.1776722\ttotal: 379ms\tremaining: 15.8ms\n",
      "96:\tlearn: 0.1766270\ttotal: 382ms\tremaining: 11.8ms\n",
      "97:\tlearn: 0.1754325\ttotal: 385ms\tremaining: 7.86ms\n",
      "98:\tlearn: 0.1750622\ttotal: 388ms\tremaining: 3.92ms\n",
      "99:\tlearn: 0.1746579\ttotal: 392ms\tremaining: 0us\n",
      "CatBoostClassifier Accuracy: 0.885\n",
      "Stochastic Gradient Boosting Classifier Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generating a synthetic classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", accuracy_gb)\n",
    "\n",
    "# LightGBM Classifier\n",
    "lgb_classifier =LGBMClassifier(n_estimators=100, random_state=42)\n",
    "lgb_classifier.fit(X_train, y_train)\n",
    "y_pred_lgb = lgb_classifier.predict(X_test)\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "print(\"LGBM Classifier Accuracy:\", accuracy_lgb)\n",
    "\n",
    "# CatBoostClassifier\n",
    "cat_classifier =CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "cat_classifier.fit(X_train, y_train)\n",
    "y_pred_cat =  cat_classifier.predict(X_test)\n",
    "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
    "print(\"CatBoostClassifier Accuracy:\", accuracy_cat)\n",
    "\n",
    "# Stochastic Gradient Boosting classifier\n",
    "stoch_gb_classifier = HistGradientBoostingClassifier(max_iter = 100 , random_state=42)\n",
    "stoch_gb_classifier.fit(X_train, y_train)\n",
    "y_pred_stoch_gb = stoch_gb_classifier.predict(X_test)\n",
    "accuracy_stoch_gb = accuracy_score(y_test, y_pred_stoch_gb)\n",
    "print(\"Stochastic Gradient Boosting Classifier Accuracy:\", accuracy_stoch_gb)\n",
    "\n",
    "\n",
    "# Gradient Boosting Regressor (for demonstration purposes)\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "y_pred_gb_regressor = gb_regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b5fde-f803-4312-9adc-3ebb11297543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
